# **Анализ алгоритма: Сортировка выбором (Selection Sort)**

**Определение:**
Сортировка выбором — это простой алгоритм сортировки, который на каждом шаге **находит минимальный элемент** в неотсортированной части массива и **меняет его местами** с первым элементом этой части.
После каждой итерации **левая часть массива становится отсортированной**.

**Ход работы:**
1. Алгоритм проходит по всему массиву.
2. Для каждой позиции `i` ищет **минимальный элемент** в правой (неотсортированной) части массива.
3. Меняет найденный минимум местами с элементом на позиции `i`.
4. Повторяет шаги, пока весь массив не станет отсортированным.

**Пошаговое объяснение:**
На примере массива `[64, 25, 12, 22, 11]`:
1. Находим минимум (11) и ставим его на первое место → `[11, 25, 12, 22, 64]`
2. Из оставшихся `[25, 12, 22, 64]` минимум — 12 → `[11, 12, 25, 22, 64]`
3. Далее минимум 22 → `[11, 12, 22, 25, 64]`
4. Последние два элемента уже на своих местах.

**Анализ:**
* Алгоритм **не использует дополнительные массивы**, выполняется **на месте (in-place)**.
* Количество сравнений всегда одинаковое — независимо от исходных данных.
* Обменов элементов — максимум `n - 1`.

**Временная сложность:**
O(n²)

**Почему O(n²):**
Для каждого элемента массива алгоритм ищет минимальный элемент в оставшейся части массива. На первом шаге сравниваем n-1 элементов, на втором — n-2, и так далее.
Общее количество сравнений: (n-1) + (n-2) + ... + 1 = n(n-1)/2 ≈ O(n²). Даже если массив почти отсортирован, нужно пройти всю неотсортированную часть, поэтому O(n²).

---

# **Анализ алгоритма: Сортировка обменом (пузырьком) (Bubble Sort)**

**Определение:**
Сортировка пузырьком — это простой алгоритм сортировки, который многократно проходит по массиву, сравнивая пары соседних элементов и меняя их местами, если они расположены в неправильном порядке.
На каждом проходе самый крупный элемент «всплывает» в конец массива, как пузырёк в воде — отсюда и название метода.

**Ход работы:**
1. Проходим по массиву от начала до конца.
2. Сравниваем каждый элемент с его соседом справа (`arr[j]` и `arr[j + 1]`).
3. Если левый элемент больше правого — меняем их местами.
4. После первого прохода самый большой элемент оказывается в конце.
5. На следующем проходе игнорируем последний отсортированный элемент и повторяем процесс.
6. Если на каком-то проходе не было ни одной перестановки — массив уже отсортирован, цикл можно завершить досрочно.

**Анализ:**
* Всего выполняется до **(n – 1)** проходов.
* На первом проходе — **(n – 1)** сравнений, на втором — **(n – 2)**, и так далее.
* В сумме это около **n(n – 1)/2 сравнений**.
* Количество обменов зависит от исходного порядка элементов:

  * при обратной сортировке — почти каждое сравнение вызывает обмен;
  * при почти отсортированном массиве — минимум обменов.

**Временная сложность**
Лучший случай O(n): если массив уже отсортирован, внутренний цикл не выполняет обменов, достаточно одного прохода.
Средний и худший O(n²): нужно сравнить почти все элементы на каждом проходе, обмены могут выполняться при каждом сравнении.

**Почему O(n²) и O(n):**
Алгоритм многократно проходит по массиву и сравнивает каждую пару соседних элементов. На первом проходе n-1 сравнений, на втором n-2 и так далее → суммарно n(n-1)/2 сравнений. Лучший случай O(n): если массив уже отсортирован, внутренний цикл не выполняет обменов, и алгоритм завершает работу после одного прохода. Худший O(n²): массив в обратном порядке, каждый элемент приходится обменивать на каждом шаге.

---

# **Анализ алгоритма: Сортировка вставками (Insertion Sort)**

**Определение:**
Сортировка вставками — это алгоритм, который формирует отсортированную последовательность, поочередно вставляя каждый элемент массива на своё место.
Принцип работы напоминает сортировку карт в руке: берём по одной карте и вставляем её в нужное место среди уже упорядоченных.

**Ход работы:**
1. Начинаем со второго элемента массива (`i = 1`), считая, что первый элемент уже отсортирован.
2. Сохраняем текущий элемент в переменную `key`.
3. Сравниваем `key` с элементами, стоящими слева от него.
4. Пока предыдущий элемент больше `key`, сдвигаем его вправо.
5. Когда найдено место, где элемент меньше или равен `key`, вставляем `key` на это место.
6. Повторяем процесс для всех элементов массива.

**Анализ:**
* Каждый элемент вставляется в уже отсортированную часть массива.
* В худшем случае (обратный порядок) — выполняется максимальное количество сравнений и сдвигов.
* В лучшем случае (уже отсортированный массив) — внутренняя проверка выполняется только один раз на элемент.

**Временная сложность**
* Лучший O(n): Если массив уже отсортирован. Элементы уже почти на месте, внутренняя проверка выполняется один раз для каждого элемента.
* Средний и худший O(n²): Если массив отсортирован в обратном порядке. Каждый элемент может сдвигать все предыдущие элементы, поэтому число операций растёт квадратично.

**Почему O(n²) и O(n):**
Каждый элемент сравнивается с элементами отсортированной части массива и сдвигает их вправо, чтобы вставить на правильное место. В среднем каждый элемент сравнивается с половиной отсортированных элементов → O(n²). Лучший случай O(n): элементы уже отсортированы, внутренний цикл почти не выполняется. Худший O(n²): обратный порядок, каждый элемент приходится сдвигать через всю отсортированную часть.

---

# **Анализ алгоритма: Сортировка слиянием (Merge Sort)**

**Определение:**
Сортировка слиянием — это **эффективный алгоритм сортировки «разделяй и властвуй»**, который рекурсивно делит массив на две части, сортирует их отдельно, а затем **объединяет (сливает)** в один отсортированный массив.

**Ход работы:**
1. **Разделение:**
   * Массив делится на две половины — левую и правую.
   * Рекурсивно вызывается сортировка для каждой половины, пока размер подмассива не станет равен 1.

2. **Слияние:**
   * После сортировки подмассивов вызывается функция `merge()`, которая объединяет два отсортированных подмассива в один общий.
   * Для этого создаются временные массивы `L[]` и `R[]`, в которые копируются данные.
   * Элементы из `L` и `R` последовательно сравниваются и помещаются обратно в исходный массив в порядке возрастания.

3. **Повторение:**
   * Процесс продолжается до тех пор, пока весь массив не будет отсортирован.

**Анализ:**
* На каждом уровне рекурсии массив делится пополам, а процесс слияния требует **O(n)** операций.
* Количество уровней рекурсии — **O(log n)** (каждое деление уменьшает размер массива вдвое).
* Таким образом, общая временная сложность — **O(n log n)**.

**Временная сложность:**
O(n log n) всегда: массив делится пополам log n раз, слияние каждой пары подмассивов требует O(n) операций.

**Почему O(n log n):**
Массив рекурсивно делится пополам, что создаёт log n уровней рекурсии. На каждом уровне выполняется слияние двух подмассивов, для которого нужно обработать все n элементов. Общее количество операций: n * log n → O(n log n) для любого случая.

---

# **Анализ алгоритма: Сортировка Шелла (Shell Sort)**

**Определение:**
Сортировка Шелла — это **улучшенная версия сортировки вставками**, которая сначала сравнивает элементы, находящиеся на некотором расстоянии (`gap`), а затем постепенно уменьшает это расстояние до 1.
Цель — переместить элементы ближе к их окончательным позициям, что уменьшает количество сдвигов при финальной сортировке вставками.

**Ход работы:**
1. Выбираем начальный шаг `gap = n // 2`, где `n` — длина массива.
2. Для каждого `gap` выполняем **сортировку вставками для элементов, отстоящих друг от друга на `gap`**.
3. Элементы сдвигаются вперёд, если они больше текущего элемента `temp`.
4. После прохождения всех элементов уменьшаем `gap` вдвое (`gap //= 2`) и повторяем процесс.
5. Когда `gap = 1`, выполняется обычная сортировка вставками, но большинство элементов уже близки к правильной позиции, что ускоряет работу.

**Анализ:**
* Сначала сортируются элементы на больших расстояниях, что позволяет быстрее устранять "длинные" беспорядки.
* На каждом шаге массив становится все более упорядоченным, что уменьшает количество сравнений и сдвигов при финальной вставке.

**Временная сложность:**
* Лучший O(n log n): при удачной последовательности шагов большинство элементов уже близко к месту назначения.
* Средний ≈ O(n^1.25): элементы частично упорядочены, меньшие сдвиги на каждом этапе.
* Худший O(n²): при плохой последовательности gap, почти все элементы сдвигаются каждый раз.

**Почему O(n^1.25) — O(n²):**
Элементы сначала сортируются на больших расстояниях (gap), затем gap уменьшается, пока не станет 1. При удачном выборе последовательности gap большинство элементов уже почти на месте → меньше сравнений и сдвигов. При плохой последовательности gap элементы сдвигаются почти каждый раз → O(n²).

---

# **Анализ алгоритма: Быстрая сортировка (Quick Sort)**

**Определение:**
Быстрая сортировка — это эффективный алгоритм, основанный на принципе **«разделяй и властвуй»**.
Он выбирает **опорный элемент (pivot)**, затем разделяет массив на две части:

* элементы **меньше или равные** опорному,
* элементы **больше** опорного.
  После этого сортировка рекурсивно применяется к обеим частям.

**Ход работы:**
1. Выбирается **опорный элемент** (`pivot`), в данном коде — последний элемент массива.
2. В функции `partition()` все элементы, меньшие или равные `pivot`, перемещаются в левую часть, а большие — в правую.
3. После разделения `pivot` ставится на своё окончательное место в отсортированном массиве.
4. Рекурсивно выполняется сортировка для левой и правой частей массива.
5. Процесс повторяется, пока каждая часть не станет из одного элемента (что означает, что она отсортирована).

**Анализ:**
* Разделение массива (`partition`) требует **O(n)** операций.
* Каждый рекурсивный вызов делит массив на две части, что даёт **O(log n)** уровней рекурсии при равномерном разделении.
* Однако при неудачном выборе опорного элемента (например, если массив уже отсортирован) разделение может быть неравномерным, что увеличивает количество рекурсий.

**Временная сложность:**
* Лучший и средний O(n log n): каждый раз массив делится примерно пополам, каждая операция partition требует O(n).
* Худший O(n²): неудачный выбор pivot (например, первый или последний элемент в отсортированном массиве), деление неравномерное.

**Почему O(n log n) и O(n²):**
Разделение массива (partition) требует O(n) операций. При удачном выборе pivot массив делится примерно пополам на каждом уровне → log n уровней → O(n log n). Худший случай O(n²): если pivot всегда выбирается крайне (например, первый элемент в отсортированном массиве), деление неравномерное, рекурсий почти n, каждая partition O(n).

---

# **Анализ алгоритма: Сортировка кучей (Heap Sort)**

**Определение:**
Сортировка кучей (Heap Sort) — это алгоритм сортировки, основанный на структуре данных **бинарная куча (heap)**. Он сначала преобразует массив в **максимальную кучу** (max-heap), где каждый родительский элемент больше своих потомков. Затем последовательно извлекает наибольший элемент (корень кучи) и помещает его в конец массива, уменьшая размер кучи до тех пор, пока весь массив не будет отсортирован.

**Ход работы:**
1. **Построение max-кучи:**

   * Последовательно вызывается функция `heapify()` для всех узлов, начиная с середины массива.
   * После этого наибольший элемент оказывается в корне кучи (в начале массива).
2. **Извлечение элементов:**

   * Меняем местами корень (наибольший элемент) и последний элемент кучи.
   * Уменьшаем размер кучи на 1.
   * Вызываем `heapify()` для корня, чтобы восстановить свойство max-кучи.
3. Повторяем шаг 2, пока все элементы не будут отсортированы.

**Анализ:**
* Алгоритм состоит из двух основных этапов: построение кучи и многократное извлечение элементов.
* Построение кучи требует **O(n)** операций.
* Каждый вызов `heapify()` занимает **O(log n)**, а выполняется он для каждого из `n` элементов.
* Таким образом, общая сложность — **O(n log n)**.
* Алгоритм **не требует дополнительной памяти**, так как сортировка выполняется внутри массива.

**Временная сложность:**
O(n log n) всегда: построение кучи требует O(n), каждая операция извлечения максимума требует O(log n), таких операций n.

**Почему O(n log n):**
Строим max-кучу: каждый элемент heapify требуется O(log n), но построение всей кучи выполняется за O(n). Извлечение n элементов из кучи: каждая операция O(log n) → n * log n. Суммарно: O(n + n log n) ≈ O(n log n) для всех случаев.

---

# **Анализ алгоритма: Линейный поиск (Linear Search)**

**Определение:**
Линейный поиск — это простой алгоритм, который последовательно просматривает каждый элемент массива, пока не найдет искомое значение или не дойдет до конца списка.

**Ход работы:**
1. Начинаем просмотр массива с первого элемента.
2. Сравниваем текущий элемент с искомым значением (`target`).
3. Если элемент совпадает с искомым — возвращаем его индекс.
4. Если элемент не совпадает, переходим к следующему.
5. Если просмотрен весь массив и элемент не найден — возвращаем `-1`.

**Анализ:**
* Алгоритм выполняет **n сравнений** в худшем случае (где `n` — количество элементов массива).
* В **лучшем случае** элемент находится сразу на первой позиции.
* В **среднем случае** элемент находится примерно в середине массива, то есть выполняется около `n/2` сравнений.
* Количество итераций линейно зависит от размера массива.

**Временная сложность:**
* **Лучший случай:** O(1) — элемент найден сразу.
* **Средний случай:** O(n/2) ≈ O(n). В среднем приходится проверить половину массива.
* **Худший случай:** O(n) — элемент последний или отсутствует, проверяем все элементы.

**Почему O(n/2) и O(n):**
Каждый элемент массива проверяется один за другим, пока не найдём target или не пройдём весь массив. Средний O(n/2) ≈ O(n): ищем в среднем половину массива. Худший O(n): элемент последний или отсутствует.

---

# **Анализ алгоритма: Двоичный поиск (Binary Search)**

**Определение:**
Двоичный поиск — это алгоритм поиска элемента в **отсортированном массиве**, который многократно делит массив пополам и сравнивает искомое значение с серединным элементом.
Он значительно быстрее линейного поиска, особенно на больших массивах.

**Ход работы:**
1. Устанавливаем левую (`left = 0`) и правую (`right = n-1`) границы массива.
2. Вычисляем середину: `mid = left + (right - left)/2`.
3. Сравниваем `arr[mid]` с искомым элементом `target`:

   * Если равны — элемент найден, возвращаем индекс.
   * Если `arr[mid] < target` — ищем в правой половине (`left = mid + 1`).
   * Если `arr[mid] > target` — ищем в левой половине (`right = mid - 1`).
4. Повторяем шаги 2–3, пока `left <= right`.
5. Если границы пересеклись и элемент не найден — возвращаем `-1`.

**Анализ:**
* Каждый раз размер области поиска **уменьшается вдвое**, что позволяет быстро находить элемент.
* Работает только с **отсортированными массивами**.

**Временная сложность:**
O(log n) в среднем и худшем: каждый шаг делит область поиска пополам, пока не найдём элемент или границы не пересекутся.

**Почему O(log n):**
Каждый раз делим область поиска пополам, сравниваем с серединным элементом. После k шагов размер области становится n / 2^k. Когда n / 2^k = 1 → k = log₂ n шагов → O(log n).

---

# **Анализ алгоритма: Интерполяционный поиск (Interpolation Search)**

**Определение:**
Интерполяционный поиск — это усовершенствованный вариант бинарного поиска, который работает быстрее на **равномерно распределённых** отсортированных массивах.
Он не делит диапазон пополам, как бинарный поиск, а **оценивает предполагаемую позицию** элемента на основе значения ключа и распределения данных (использует формулу интерполяции).

**Ход работы:**=
1. Устанавливаются начальные границы: левая (`lo = 0`) и правая (`hi = n - 1`).
2. Пока `x` находится в диапазоне `[arr[lo], arr[hi]]`, вычисляется **предполагаемая позиция** `pos` по формуле:
   [
   pos = lo + \frac{(x - arr[lo]) \times (hi - lo)}{arr[hi] - arr[lo]}
   ]
3. Проверяется элемент на позиции `pos`:
   * Если `arr[pos] == x`, элемент найден.
   * Если `arr[pos] < x`, поиск продолжается в правой части (`lo = pos + 1`).
   * Если `arr[pos] > x`, поиск продолжается в левой части (`hi = pos - 1`).
4. Если границы пересеклись или элемент выходит за диапазон — поиск завершается.

**Анализ:**
* В отличие от бинарного поиска, который всегда выбирает середину, интерполяционный поиск подбирает позицию **ближе к реальному значению** искомого элемента.
* Эффективен, если элементы массива распределены **равномерно**.
* При неравномерном распределении эффективность может снижаться.

**Временная сложность:**
* Лучший O(1): элемент сразу на предполагаемой позиции.
* Средний O(log log n): при равномерном распределении поиск делит область ближе к целевому элементу.
* Худший O(n): при неравномерном распределении, деление может быть малоэффективным, почти как линейный поиск.

**Почему O(log log n) — O(n):**
Элементы предполагаются равномерно распределёнными, вычисляем позицию приближённо через формулу интерполяции. Каждый шаг сокращает область поиска ближе к target. Средний случай O(log log n): при равномерном распределении. Худший O(n): при неравномерном распределении почти линейный обход. Лучший O(1): target на предполагаемой позиции.

---

# **Анализ алгоритма: Поиск Фибоначчи (Fibonacci Search)**

**Определение:**
Поиск Фибоначчи — это алгоритм поиска элемента в **отсортированном массиве**, основанный на использовании **чисел Фибоначчи** для деления массива на части. Он похож на бинарный поиск, но вместо деления пополам использует деление по индексам, связанным с последовательностью Фибоначчи. Это помогает избежать деления и эффективно работать на структурах данных с неравномерным доступом.

**Ход работы:**
1. **Генерация последовательности Фибоначчи**, пока число не превысит размер массива.
2. Определяется наибольшее число Фибоначчи, меньшее или равное длине массива (`fib_m`).
3. Устанавливается **смещение** (`offset = -1`) для отслеживания сдвига начала диапазона поиска.
4. Пока индекс Фибоначчи положителен:
   * Проверяется элемент с индексом `i = min(offset + fib_{m-1}, n - 1)`.
   * Если `arr[i] < target` → сдвигаем диапазон вправо, уменьшая `fib_m` на 1.
   * Если `arr[i] > target` → сдвигаем диапазон влево, уменьшая `fib_m` на 2.
   * Если элемент найден → возвращается индекс.
5. Если элемент не найден — возвращается `-1`.

**Анализ:**
* Алгоритм эффективно сокращает область поиска, используя последовательность Фибоначчи, аналогично бинарному поиску.
* Деления заменены на **сложения и вычитания**, что повышает эффективность в некоторых вычислительных системах.
* Требуется, чтобы массив был **отсортирован**.

**Временная сложность:**
O(log n): последовательность Фибоначчи делит диапазон аналогично бинарному поиску, количество сравнений растёт логарифмически.

**Почему O(log n):**
Использует числа Фибоначчи для деления диапазона поиска. Каждое сравнение уменьшает размер области поиска примерно на пропорцию числа Фибоначчи → аналогично бинарному поиску. Количество шагов пропорционально log n.
